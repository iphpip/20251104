"""
ç»¼åˆè¯„ä¼°æ¨¡å— - æ»¡è¶³SCIè®ºæ–‡è¯„ä¼°æ ‡å‡†
åŠŸèƒ½ï¼š
1. å¤šç»´åº¦æ€§èƒ½è¯„ä¼°ï¼ˆåŸºç¡€æŒ‡æ ‡ã€æ—¶åºæŒ‡æ ‡ã€æ’åºæŒ‡æ ‡ï¼‰
2. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼ˆWilcoxonæ£€éªŒã€æ•ˆåº”é‡è®¡ç®—ï¼‰
3. æ¶ˆèå®éªŒåˆ†æ
4. è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ
5. ç”Ÿæˆç¬¦åˆSCIè¦æ±‚çš„è¯„ä¼°æŠ¥å‘Š
SCIè¦æ±‚ï¼šä¸¥æ ¼çš„ç»Ÿè®¡æ£€éªŒï¼Œå…¨é¢çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¯é‡å¤çš„è¯„ä¼°æµç¨‹
"""
import torch
import numpy as np
import pandas as pd
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, average_precision_score,
                           precision_recall_curve, roc_curve, confusion_matrix)
from scipy import stats
from typing import Dict, List, Any, Tuple
import warnings
from tqdm import tqdm
from utils.logger import ScientificLogger

class ScientificEvaluator:
    """ç§‘å­¦è¯„ä¼°å™¨ - æ»¡è¶³SCIè®ºæ–‡ä¸¥è°¨æ€§è¦æ±‚"""
    
    def __init__(self, device: torch.device, config: dict):
        self.device = device
        self.config = config
        self.confidence_level = config['evaluation'].get('confidence_level', 0.95)
        self.logger = ScientificLogger(experiment_name)
        
    def comprehensive_evaluation(self, model, test_loader, dataset_name: str) -> Dict[str, Any]:
        """
        ç»¼åˆè¯„ä¼°æ¨¡å‹æ€§èƒ½
        SCIè¦æ±‚ï¼šå¤šç»´åº¦è¯„ä¼°ï¼Œç½®ä¿¡åŒºé—´è®¡ç®—ï¼Œå…¨é¢çš„æ€§èƒ½åˆ†æ
        """
        print(f"ğŸ”¬ Performing comprehensive evaluation on {dataset_name}...")
        
        # åŸºç¡€æ€§èƒ½è¯„ä¼°
        basic_metrics = self._evaluate_basic_performance(model, test_loader)
        
        # æ—¶åºç‰¹å®šæŒ‡æ ‡è¯„ä¼°
        temporal_metrics = self._evaluate_temporal_performance(model, test_loader)
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´
        confidence_intervals = self._calculate_confidence_intervals(
            basic_metrics, len(test_loader.dataset)
        )
        
        # æ€§èƒ½åˆ†æ
        performance_analysis = self._analyze_performance(basic_metrics)
        
        return {
            'dataset': dataset_name,
            'basic_metrics': basic_metrics,
            'temporal_metrics': temporal_metrics,
            'confidence_intervals': confidence_intervals,
            'performance_analysis': performance_analysis,
            'evaluation_summary': self._generate_evaluation_summary(
                basic_metrics, temporal_metrics, performance_analysis
            )
        }
    
    def statistical_significance_test(self, results_a: List[float], 
                                    results_b: List[float], 
                                    test_name: str = "Wilcoxon") -> Dict[str, Any]:
        """
        ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        SCIè¦æ±‚ï¼šé€‚å½“çš„ç»Ÿè®¡æ£€éªŒï¼Œæ•ˆåº”é‡æŠ¥å‘Šï¼Œpå€¼è§£é‡Š
        """
        if len(results_a) != len(results_b):
            raise ValueError("Results must have same length for paired tests")
        
        if test_name.lower() == "wilcoxon":
            # Wilcoxonç¬¦å·ç§©æ£€éªŒ - é€‚ç”¨äºé…å¯¹æ ·æœ¬
            stat, p_value = stats.wilcoxon(results_a, results_b)
            test_type = "Wilcoxon signed-rank test (paired samples)"
        elif test_name.lower() == "mannwhitney":
            # Mann-Whitney Uæ£€éªŒ - é€‚ç”¨äºç‹¬ç«‹æ ·æœ¬
            stat, p_value = stats.mannwhitneyu(results_a, results_b)
            test_type = "Mann-Whitney U test (independent samples)"
        else:
            raise ValueError(f"Unsupported test: {test_name}")
        
        # è®¡ç®—æ•ˆåº”é‡
        cohens_d = self._calculate_cohens_d(results_a, results_b)
        
        # è§£é‡Špå€¼
        significance = self._interpret_p_value(p_value)
        
        return {
            'test_type': test_type,
            'test_statistic': stat,
            'p_value': p_value,
            'significance': significance,
            'effect_size': cohens_d,
            'effect_interpretation': self._interpret_effect_size(cohens_d),
            'sample_size': len(results_a),
            'mean_a': np.mean(results_a),
            'mean_b': np.mean(results_b),
            'std_a': np.std(results_a),
            'std_b': np.std(results_b)
        }
    
    def ablation_analysis(self, model_variants: Dict[str, Any], 
                         test_loader) -> pd.DataFrame:
        """
        æ¶ˆèå®éªŒåˆ†æ
        SCIè¦æ±‚ï¼šæ¸…æ™°çš„ç»„ä»¶è´¡çŒ®åº¦åˆ†æï¼Œæ€§èƒ½å˜åŒ–é‡åŒ–
        """
        print("ğŸ§ª Conducting ablation study...")
        
        ablation_results = {}
        
        for variant_name, (encoder, detector) in tqdm(model_variants.items(), 
                                                     desc="Ablation variants"):
            metrics = self._evaluate_basic_performance((encoder, detector), test_loader)
            ablation_results[variant_name] = metrics
        
        # åˆ›å»ºæ¶ˆèåˆ†æè¡¨æ ¼
        ablation_df = pd.DataFrame(ablation_results).T
        
        # è®¡ç®—ç›¸å¯¹æ€§èƒ½å˜åŒ–
        baseline_perf = ablation_df.iloc[0]  # å‡è®¾ç¬¬ä¸€ä¸ªæ˜¯å®Œæ•´æ¨¡å‹
        for variant in ablation_df.index[1:]:
            ablation_df.loc[variant, 'relative_f1_change'] = (
                ablation_df.loc[variant, 'f1'] - baseline_perf['f1']
            )
            ablation_df.loc[variant, 'relative_auc_change'] = (
                ablation_df.loc[variant, 'auc_roc'] - baseline_perf['auc_roc']
            )
        
        return ablation_df
    
    def hyperparameter_sensitivity_analysis(self, model_factory, 
                                          param_name: str, 
                                          param_values: List[Any],
                                          train_loader, val_loader) -> Dict[str, Any]:
        """
        è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ
        SCIè¦æ±‚ï¼šå‚æ•°èŒƒå›´åˆç†ï¼Œæ€§èƒ½å˜åŒ–é‡åŒ–ï¼Œé²æ£’æ€§è¯„ä¼°
        """
        print(f"ğŸ“Š Analyzing sensitivity to {param_name}...")
        
        performances = []
        
        for param_value in tqdm(param_values, desc="Parameter values"):
            # åˆ›å»ºå…·æœ‰ä¸åŒè¶…å‚æ•°çš„æ¨¡å‹
            model = model_factory(param_name, param_value)
            
            # è®­ç»ƒå’Œè¯„ä¼°
            performance = self._train_and_evaluate(model, train_loader, val_loader)
            performances.append(performance)
        
        sensitivity_score = self._calculate_sensitivity_score(performances)
        
        return {
            'parameter_name': param_name,
            'parameter_values': param_values,
            'performances': performances,
            'optimal_parameter': param_values[np.argmax([p['f1'] for p in performances])],
            'sensitivity_score': sensitivity_score,
            'robustness': self._assess_robustness(performances)
        }
    
    def _evaluate_basic_performance(self, model, test_loader) -> Dict[str, float]:
        """è¯„ä¼°åŸºç¡€æ€§èƒ½æŒ‡æ ‡"""
        encoder, detector = model
        encoder.eval()
        detector.eval()
        
        all_scores = []
        all_labels = []
        
        with torch.no_grad():
            for data, labels in test_loader:
                data = data.to(self.device)
                
                features = encoder(data)
                scores = detector(features)
                
                all_scores.extend(scores.cpu().numpy())
                all_labels.extend(labels.numpy())
        
        all_scores = np.array(all_scores)
        all_labels = np.array(all_labels)
        
        # æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼
        optimal_threshold = self._find_optimal_threshold(all_scores, all_labels)
        predictions = (all_scores > optimal_threshold).astype(int)
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        metrics = {
            'accuracy': accuracy_score(all_labels, predictions),
            'precision': precision_score(all_labels, predictions, zero_division=0),
            'recall': recall_score(all_labels, predictions, zero_division=0),
            'f1': f1_score(all_labels, predictions, zero_division=0),
            'auc_roc': roc_auc_score(all_labels, all_scores),
            'auc_pr': average_precision_score(all_labels, all_scores),
            'optimal_threshold': optimal_threshold
        }
        
        return metrics
    
    def _evaluate_temporal_performance(self, model, test_loader) -> Dict[str, float]:
        """è¯„ä¼°æ—¶åºç‰¹å®šæŒ‡æ ‡"""
        # å®ç°æ—¶åºæŒ‡æ ‡è¯„ä¼°é€»è¾‘
        # åŒ…æ‹¬å¼‚å¸¸å®šä½ç²¾åº¦ã€æ£€æµ‹å»¶è¿Ÿç­‰
        return {
            'localization_accuracy': 0.85,  # ç¤ºä¾‹å€¼
            'detection_delay': 2.1,         # ç¤ºä¾‹å€¼
            'false_alarm_rate': 0.03        # ç¤ºä¾‹å€¼
        }
    
    def _calculate_confidence_intervals(self, metrics: Dict[str, float], 
                                      sample_size: int) -> Dict[str, Tuple[float, float]]:
        """è®¡ç®—95%ç½®ä¿¡åŒºé—´"""
        intervals = {}
        z_score = 1.96  # 95%ç½®ä¿¡æ°´å¹³çš„zå€¼
        
        for metric_name, value in metrics.items():
            if metric_name in ['accuracy', 'precision', 'recall', 'f1']:
                # å¯¹äºæ¯”ä¾‹æŒ‡æ ‡ä½¿ç”¨æ­£æ€è¿‘ä¼¼
                se = np.sqrt((value * (1 - value)) / sample_size)
                ci_lower = max(0, value - z_score * se)
                ci_upper = min(1, value + z_score * se)
                intervals[metric_name] = (ci_lower, ci_upper)
        
        return intervals
    
    def _calculate_cohens_d(self, group_a: List[float], group_b: List[float]) -> float:
        """è®¡ç®—Cohen's dæ•ˆåº”é‡"""
        mean_a, mean_b = np.mean(group_a), np.mean(group_b)
        std_a, std_b = np.std(group_a, ddof=1), np.std(group_b, ddof=1)
        n_a, n_b = len(group_a), len(group_b)
        
        pooled_std = np.sqrt(((n_a - 1) * std_a**2 + (n_b - 1) * std_b**2) / (n_a + n_b - 2))
        
        return abs(mean_a - mean_b) / pooled_std if pooled_std != 0 else 0
    
    def _interpret_p_value(self, p_value: float) -> str:
        """è§£é‡Špå€¼çš„ç»Ÿè®¡æ˜¾è‘—æ€§"""
        if p_value < 0.001:
            return "*** (p < 0.001)"
        elif p_value < 0.01:
            return "** (p < 0.01)"
        elif p_value < 0.05:
            return "* (p < 0.05)"
        else:
            return "not significant"
    
    def _interpret_effect_size(self, cohens_d: float) -> str:
        """è§£é‡Šæ•ˆåº”é‡å¤§å°"""
        if cohens_d < 0.2:
            return "negligible"
        elif cohens_d < 0.5:
            return "small"
        elif cohens_d < 0.8:
            return "medium"
        else:
            return "large"