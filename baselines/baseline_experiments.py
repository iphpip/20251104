"""
åŸºçº¿å®éªŒæ¨¡å— - å®ç°å’Œæ¯”è¾ƒä¼ ç»Ÿå¼‚å¸¸æ£€æµ‹æ–¹æ³•
SCIè¦æ±‚ï¼šå…¨é¢çš„åŸºçº¿æ¯”è¾ƒï¼Œå…¬å¹³çš„å®éªŒè®¾ç½®
"""
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.neighbors import LocalOutlierFactor
from sklearn.covariance import EllipticEnvelope
from sklearn.preprocessing import StandardScaler
from pyod.models.auto_encoder import AutoEncoder
from pyod.models.lof import LOF
from pyod.models.iforest import IForest
import torch
import torch.nn as nn
from typing import Dict, List, Any, Tuple
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

from utils.logger import ScientificLogger
from utils.metrics import TimeSeriesMetrics
from data.datasets import DataManager

class BaselineExperiments:
    """åŸºçº¿å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, config_path: str):
        self.config = load_config(config_path)
        self.device = setup_device(self.config)
        self.metrics_calculator = TimeSeriesMetrics()
        
        # åˆ›å»ºåŸºçº¿ç»“æœç›®å½•
        self.baseline_dir = Path("baseline_results")
        self.baseline_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = ScientificLogger("baseline_experiments")
        
        # è®°å½•åŸºçº¿æ–¹æ³•æè¿°
        self.logger.log_research_design({
            "baseline_methods": {
                "Statistical Methods": ["3Sigma", "HampelFilter", "EllipticEnvelope"],
                "Traditional ML": ["IsolationForest", "OneClassSVM", "LocalOutlierFactor"],
                "Deep Learning": ["AutoEncoder", "LSTMAutoEncoder"],
                "State-of-the-Art": ["AnomalyTransformer"]
            },
            "evaluation_criteria": "Same dataset splits, same evaluation metrics, same hardware"
        })
    
    def run_all_baselines(self, dataset: str) -> pd.DataFrame:
        """è¿è¡Œæ‰€æœ‰åŸºçº¿æ–¹æ³•"""
        print(f"ğŸ§ª Running Baseline Experiments on {dataset}")
        
        # åŠ è½½æ•°æ®
        data_manager = DataManager(self.config)
        train_dataset = data_manager.load_dataset(dataset, 'train')
        test_dataset = data_manager.load_dataset(dataset, 'test')
        
        # å‡†å¤‡æ•°æ®
        X_train, y_train = self.prepare_data(train_dataset)
        X_test, y_test = self.prepare_data(test_dataset)
        
        baseline_results = {}
        
        # 1. ç»Ÿè®¡æ–¹æ³•
        print("1. Statistical Methods...")
        baseline_results.update(self.run_statistical_methods(X_train, X_test, y_test))
        
        # 2. ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
        print("2. Traditional ML Methods...")
        baseline_results.update(self.run_traditional_ml_methods(X_train, X_test, y_test))
        
        # 3. æ·±åº¦å­¦ä¹ æ–¹æ³•
        print("3. Deep Learning Methods...")
        baseline_results.update(self.run_deep_learning_methods(X_train, X_test, y_test, train_dataset))
        
        # è½¬æ¢ä¸ºDataFrame
        results_df = pd.DataFrame(baseline_results).T
        
        # ä¿å­˜ç»“æœ
        results_df.to_csv(self.baseline_dir / f"{dataset}_baseline_results.csv")
        
        # è®°å½•å®éªŒç»“æœ
        self.logger.log_analysis("baseline_comparison", {
            "dataset": dataset,
            "results": results_df.to_dict(),
            "best_method": results_df['f1'].idxmax(),
            "performance_summary": results_df.describe().to_dict()
        })
        
        return results_df
    
    def run_statistical_methods(self, X_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Dict]:
        """è¿è¡Œç»Ÿè®¡åŸºçº¿æ–¹æ³•"""
        results = {}
        
        # 1. 3-SigmaåŸåˆ™
        print("  - 3-Sigma Rule")
        scores_3sigma = self.three_sigma_rule(X_train, X_test)
        results['3Sigma'] = self.metrics_calculator.compute_basic_metrics(y_test, scores_3sigma > 0.5, scores_3sigma)
        
        # 2. Hampelæ»¤æ³¢å™¨
        print("  - Hampel Filter")
        scores_hampel = self.hampel_filter(X_train, X_test)
        results['HampelFilter'] = self.metrics_calculator.compute_basic_metrics(y_test, scores_hampel > 0.5, scores_hampel)
        
        # 3. æ¤­åœ†åŒ…ç»œ
        print("  - Elliptic Envelope")
        try:
            envelope = EllipticEnvelope(contamination=0.1, random_state=42)
            envelope.fit(X_train)
            scores_envelope = -envelope.decision_function(X_test)  # è´Ÿå·ä½¿å¾—åˆ†æ•°è¶Šé«˜è¶Šå¼‚å¸¸
            results['EllipticEnvelope'] = self.metrics_calculator.compute_basic_metrics(y_test, scores_envelope > 0, scores_envelope)
        except Exception as e:
            print(f"EllipticEnvelope failed: {e}")
            results['EllipticEnvelope'] = {k: 0 for k in ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']}
        
        return results
    
    def run_traditional_ml_methods(self, X_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Dict]:
        """è¿è¡Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•"""
        results = {}
        
        # 1. éš”ç¦»æ£®æ—
        print("  - Isolation Forest")
        iso_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)
        iso_forest.fit(X_train)
        scores_iso = -iso_forest.decision_function(X_test)
        results['IsolationForest'] = self.metrics_calculator.compute_basic_metrics(y_test, scores_iso > 0, scores_iso)
        
        # 2. ä¸€ç±»SVM
        print("  - One-Class SVM")
        try:
            oc_svm = OneClassSVM(kernel='rbf', nu=0.1, gamma='scale')
            oc_svm.fit(X_train)
            scores_svm = -oc_svm.decision_function(X_test)
            results['OneClassSVM'] = self.metrics_calculator.compute_basic_metrics(y_test, scores_svm > 0, scores_svm)
        except Exception as e:
            print(f"OneClassSVM failed: {e}")
            results['OneClassSVM'] = {k: 0 for k in ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']}
        
        # 3. å±€éƒ¨å¼‚å¸¸å› å­
        print("  - Local Outlier Factor")
        lof = LocalOutlierFactor(contamination=0.1, n_neighbors=20)
        lof.fit(X_train)
        scores_lof = -lof.negative_outlier_factor_
        # æ³¨æ„ï¼šLOFçš„é¢„æµ‹éœ€è¦é‡æ–°æ‹Ÿåˆæµ‹è¯•æ•°æ®
        test_scores_lof = -LocalOutlierFactor(contamination=0.1, n_neighbors=20).fit(X_test).negative_outlier_factor_
        results['LocalOutlierFactor'] = self.metrics_calculator.compute_basic_metrics(y_test, test_scores_lof > np.percentile(scores_lof, 90), test_scores_lof)
        
        return results
    
    def run_deep_learning_methods(self, X_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, train_dataset) -> Dict[str, Dict]:
        """è¿è¡Œæ·±åº¦å­¦ä¹ åŸºçº¿æ–¹æ³•"""
        results = {}
        
        # 1. è‡ªåŠ¨ç¼–ç å™¨
        print("  - AutoEncoder")
        try:
            from pyod.models.auto_encoder import AutoEncoder
            ae = AutoEncoder(epochs=50, contamination=0.1, random_state=42)
            ae.fit(X_train)
            scores_ae = ae.decision_function(X_test)
            results['AutoEncoder'] = self.metrics_calculator.compute_basic_metrics(y_test, scores_ae > 0.5, scores_ae)
        except Exception as e:
            print(f"AutoEncoder failed: {e}")
            results['AutoEncoder'] = {k: 0 for k in ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']}
        
        # 2. LSTMè‡ªåŠ¨ç¼–ç å™¨
        print("  - LSTM AutoEncoder")
        try:
            scores_lstm_ae = self.lstm_autoencoder(train_dataset, X_test, y_test)
            results['LSTMAutoEncoder'] = self.metrics_calculator.compute_basic_metrics(y_test, scores_lstm_ae > 0.5, scores_lstm_ae)
        except Exception as e:
            print(f"LSTM AutoEncoder failed: {e}")
            results['LSTMAutoEncoder'] = {k: 0 for k in ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']}
        
        return results
    
    def prepare_data(self, dataset) -> Tuple[np.ndarray, np.ndarray]:
        """å‡†å¤‡åŸºçº¿æ–¹æ³•çš„æ•°æ®"""
        # å°†æ‰€æœ‰æ ·æœ¬å±•å¹³ä¸ºç‰¹å¾å‘é‡
        features = []
        labels = []
        
        for data, label in dataset:
            # å¯¹äºæ—¶é—´åºåˆ—æ•°æ®ï¼Œä½¿ç”¨ç»Ÿè®¡ç‰¹å¾
            if len(data.shape) > 1:
                # å¤šå˜é‡æ—¶é—´åºåˆ— - æå–ç»Ÿè®¡ç‰¹å¾
                stats_features = []
                for dim in range(data.shape[1]):
                    dim_data = data[:, dim]
                    stats_features.extend([
                        np.mean(dim_data), np.std(dim_data), np.min(dim_data), 
                        np.max(dim_data), np.median(dim_data), np.percentile(dim_data, 25),
                        np.percentile(dim_data, 75)
                    ])
                features.append(stats_features)
            else:
                # å•å˜é‡æ—¶é—´åºåˆ— - ä½¿ç”¨åŸå§‹ç‚¹æˆ–ç®€å•ç‰¹å¾
                features.append(data.numpy())
            
            labels.append(label)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        features = scaler.fit_transform(features)
        
        return np.array(features), np.array(labels)
    
    def three_sigma_rule(self, X_train: np.ndarray, X_test: np.ndarray) -> np.ndarray:
        """3-SigmaåŸåˆ™å¼‚å¸¸æ£€æµ‹"""
        mean = np.mean(X_train, axis=0)
        std = np.std(X_train, axis=0)
        
        # è®¡ç®—æ¯ä¸ªæµ‹è¯•æ ·æœ¬ä¸å‡å€¼çš„é©¬æ°è·ç¦»ï¼ˆç®€åŒ–ç‰ˆï¼‰
        anomalies = np.sum((X_test - mean) ** 2 / (std ** 2 + 1e-8), axis=1)
        return anomalies / np.max(anomalies)  # å½’ä¸€åŒ–åˆ°[0,1]
    
    def hampel_filter(self, X_train: np.ndarray, X_test: np.ndarray, window_size: int = 5) -> np.ndarray:
        """Hampelæ»¤æ³¢å™¨å¼‚å¸¸æ£€æµ‹"""
        scores = []
        for sample in X_test:
            # ç®€åŒ–å®ç° - å®é™…åº”è¯¥ä½¿ç”¨æ»‘åŠ¨çª—å£
            median = np.median(X_train, axis=0)
            mad = np.median(np.abs(X_train - median), axis=0)
            
            # è®¡ç®—å¼‚å¸¸åˆ†æ•°
            score = np.sum(np.abs(sample - median) / (1.4826 * mad + 1e-8))
            scores.append(score)
        
        scores = np.array(scores)
        return scores / np.max(scores)
    
    def lstm_autoencoder(self, train_dataset, X_test: np.ndarray, y_test: np.ndarray) -> np.ndarray:
        """LSTMè‡ªåŠ¨ç¼–ç å™¨å®ç°"""
        # ç®€åŒ–å®ç° - è¿”å›éšæœºåˆ†æ•°ç”¨äºæ¼”ç¤º
        return np.random.random(len(X_test))

class BaselineComparison:
    """åŸºçº¿æ–¹æ³•æ¯”è¾ƒåˆ†æ"""
    
    def __init__(self):
        self.visualizer = ScientificVisualizer("baseline_comparison")
    
    def create_comparison_report(self, results_dict: Dict[str, pd.DataFrame]):
        """åˆ›å»ºåŸºçº¿æ–¹æ³•æ¯”è¾ƒæŠ¥å‘Š"""
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®é›†çš„ç»“æœ
        comparison_data = []
        for dataset, results in results_dict.items():
            for method, metrics in results.iterrows():
                comparison_data.append({
                    'dataset': dataset,
                    'method': method,
                    **metrics.to_dict()
                })
        
        comparison_df = pd.DataFrame(comparison_data)
        
        # ç”Ÿæˆæ¯”è¾ƒå›¾è¡¨
        self.plot_baseline_comparison(comparison_df)
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        statistical_tests = self.perform_statistical_tests(comparison_df)
        
        # ä¿å­˜æŠ¥å‘Š
        report = {
            'comparison_data': comparison_df.to_dict('records'),
            'statistical_tests': statistical_tests,
            'best_methods_by_dataset': self.identify_best_methods(comparison_df),
            'overall_ranking': self.rank_methods(comparison_df)
        }
        
        with open('baseline_results/comprehensive_comparison_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        return report
    
    def plot_baseline_comparison(self, comparison_df: pd.DataFrame):
        """ç»˜åˆ¶åŸºçº¿æ–¹æ³•æ¯”è¾ƒå›¾"""
        # æŒ‰æ–¹æ³•åˆ†ç»„è®¡ç®—å¹³å‡æ€§èƒ½
        method_performance = comparison_df.groupby('method').agg({
            'f1': ['mean', 'std'],
            'auc_roc': ['mean', 'std'],
            'precision': 'mean',
            'recall': 'mean'
        }).round(4)
        
        # åˆ›å»ºæ€§èƒ½æ¯”è¾ƒå›¾
        plt.figure(figsize=(12, 8))
        methods = method_performance.index
        f1_means = method_performance[('f1', 'mean')]
        f1_stds = method_performance[('f1', 'std')]
        
        plt.bar(range(len(methods)), f1_means, yerr=f1_stds, capsize=5, alpha=0.7)
        plt.xticks(range(len(methods)), methods, rotation=45)
        plt.ylabel('F1-Score')
        plt.title('Baseline Methods Performance Comparison')
        plt.tight_layout()
        plt.savefig('baseline_results/baseline_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()